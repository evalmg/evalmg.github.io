---
layout: default
---

## **The 1st Evaluation of Multi-Modal Generation Workshop**

Multimodal generation techniques have opened new avenues for creative content generation.  However, evaluating the quality of multimodal generation remains underexplored and some key questions are unanswered, such as the contributions of each modal, the utility of pre-trained large language models for multimodal generation, and measuring faithfulness and fairness in multimodal outputs.  This workshop aims to foster discussions and research efforts by bringing together researchers and practitioners in natural language processing, computer vision, and multimodal AI. Our goal is to establish evaluation methods for multimodal research and advance research efforts in this direction. 

## **Call for Papers**

Both long paper and short papers (up to 8 pages and 4 pages respectively with unlimited references and appendices) are welcomed for submission. 

A list of topics relevant to this workshop (but not limited to):

- Evaluation metrics for multimodal text generation for assessing informativeness, factuality and faithfulness 

- New benchmark datasets, evaluation protocols and annotations

- Challenges in evaluating multimodal coherence, relevance and contribution of modalities and inter- and intra-interactions

- Assessing information integration and aggregation across multiple modalities 

- Adversarial evaluation approaches for testing the robustness and reliability of multimodal generation systems

- Ethical considerations in the evaluation of multimodal text generation, including bias detection and mitigation strategies

- Multilingual multimodal text generation systems for low-resource languages

- Evaluating fairness and privacy in multimodal learning and applications


## **Important Dates**

- Sep XX, 2024: Workshop Submission Due Date 

- Oct XX, 2024: Fast-Track Submission and ARR Commitment Deadline 

- Oct XX, 2024: Notification of Acceptance (Direct, ARR, and Fast-Track Notification)

- Dec XX, 2024: Camera-ready Papers Due

- Jan 19, 2025: Workshop Date

## **Submission Instructions**


You are invited to submit your papers in our [START/SoftConf submission portal](https://softconf.com/coling2025/EvalMG25). All the submitted papers have to be anonymous for double-blind review. The content of the paper should not be longer than 8 pages for long papers and 4 pages for short papers, strictly following the [ACL style templates](https://aclrollingreview.org/cfp#paper-submission-and-templates), with the mandatory limitation section not counting towards the page limit. Supplementary and appendices (either as separate files or appended after the main submission) are allowed. We encourage code link submissions for reproducibility.

## **Invited Speakers**


<table style="border-collapse: collapse; border: none; width: 100%;">
  <tr>
    <!-- Adjust width for the image column -->
    <td style="border: none; width: 30%; text-align: center;">
      <h3>Timothy Baldwin</h3>
      <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=wjBD1dkAAAAJ&citpid=8" alt="Timothy Baldwin" style="border-radius: 50%; max-width: 100%; height: auto;"><br>
    </td>
    <!-- Adjust width for the text column -->
    <td style="border: none; width: 70%; text-align: left;">
      Timothy Baldwin is a Professor at MBZUAI whose research focuses on natural language processing (NLP), including deep learning, algorithmic fairness, computational social science, and social media analytics.
    </td>
  </tr>
</table>




## **Organisers**

- Wei Emma Zhang, The University of Adelaide
- Xiang Dai, CSIRO
- Desmond Elliot, University of Copenhagen
- Byron Fang, CSIRO
- Haojie Zhuang, The University of Adelaide
- Mong Yuan Sim, The University of Adelaide

<!-- <table style="border-collapse: collapse; border: none; margin: 0 auto;">
  <tr>
    <td style="border: none;" align="center">
      <img src="https://weiezhang.github.io/images/profile.png" alt="Wei Emma Zhang" style="border-radius: 50%; width: 160px;"><br>
      <a href="https://jbdel.github.io/">Wei Emma Zhang</a>
    </td>
    <td style="border: none;" align="center">
      <img src="https://people.csiro.au/-/media/People-Finder/D/X/dai-dai/DSC09906_Edit.jpg?mh=600&mw=600&hash=6F3DFC0CB3CF01772A1132CACB0CA03734CE395E" alt="Xiang Dai" style="border-radius: 50%; width: 160px;"><br>
      <a href="https://zhjohnchan.github.io/">Xiang Dai</a>
    </td>
    <td style="border: none;" align="center">
      <img src="https://www2.adm.ku.dk/selv/pls/prt_www40.hentindhold_cms?p_personid=631668" alt="Desmond Elliot" style="border-radius: 50%; width: 160px;"><br>
      <a href="https://maya-varma.com/">Desmond Elliot</a>
    </td>
  </tr>

  <tr>
    <td style="border: none;" align="center">
      <img src="https://people.csiro.au:443/-/media/People-Finder/F/B/byron-fang/thumbnail_image_6487327.jpg?mh=600&mw=600&hash=6AFFFD77A4249611032D826E8C87D1DDF8AF3E91" alt="Byron Fang" style="border-radius: 50%; width: 160px;"><br>
      <a href="https://jbdel.github.io/">Byron Fang</a>
    </td>
    <td style="border: none;" align="center">
      <img src="" alt="Haojie Zhuang" style="border-radius: 50%; width: 160px;"><br>
      <a href="https://zhjohnchan.github.io/">Haojie Zhuang</a>
    </td>
    <td style="border: none;" align="center">
      <img src="https://media.licdn.com/dms/image/C5603AQHMp3iIWWjkFQ/profile-displayphoto-shrink_800_800/0/1644989070456?e=1725494400&v=beta&t=foHGZMwsZvKahZAhntJgMdfPhcnT2iKipCuMYcIXl1w" alt="Mong Yuan Sim" style="border-radius: 50%; width: 160px;"><br>
      <a href="https://maya-varma.com/">Mong Yuan Sim</a>
    </td>
  </tr>
</table> -->


