---
layout: default
has_previous: true
---


## **2nd Evaluation for Multimodal Generation Workshop**

<p style="text-align: justify;">
    Multimodal generation techniques have opened new avenues for creative content generation.  However, evaluating the quality of multimodal generation remains underexplored and some key questions are unanswered, such as the contributions of each modal, the utility of pre-trained large language models for multimodal generation, and measuring faithfulness and fairness in multimodal outputs.  This workshop aims to foster discussions and research efforts by bringing together researchers and practitioners in natural language processing, computer vision, and multimodal AI. Our goal is to establish evaluation methods for multimodal research and advance research efforts in this direction. 
</p>


## **Call for Papers**
<a id="call-for-papers"></a>

Both long paper and short papers (up to 8 pages and 4 pages respectively with unlimited references and appendices) are welcomed for submission. 

A list of topics relevant to this workshop (but not limited to):

- Multimodal retrieval for RAG, Agentic AI, recommendation systems

- Evaluation of retrieved cross-modal samples, without relying on augmented generation

- Multi-aspect evaluation methods capturing inter- and intra-modal coherence, relevance, grounding, and contextual consistency

- Benchmark retrieval datasets, evaluation protocols and annotations for text–image–audio–video–3D generation

- Automatic and human-centric metrics for informativeness, factuality, fluency, faithfulness, calibration, and usability for multimodal generation

- Methodology for detecting, analysing, and mitigating multimodal bias, stereotypes, toxicity, and hallucinations

- Evaluation in multimodal low-resource and multilingual settings, including culturally aware and cross-lingual metrics

- Agent-based evaluation of multimodal generation in multi-turn, tool-use, or iterative editing scenarios

- Game-theoretic or optimization-based formulations of evaluation objectives and protocols 

- Evaluation of the generation quality of synthetic multimodal data, provenance/attribution, and downstream impact on training and deployment

- Ethical considerations in the evaluation of multimodal text generation, including bias detection and mitigation strategies

- Evaluation of Security and Privacy Dimensions in Multimodal Applications

## **Important Dates**
<a id="important-dates"></a>

- Apr 17, 2026: Workshop Paper Submission

- May 21, 2026: Workshop Paper Notification

- July 24, 2026: Workshop Day

Note: All deadlines are 11:59PM UTC-12:00 (“Anywhere on Earth”)

## **Submission Instructions**
<a id="submission"></a>

<p style="text-align: justify;">
Instructions for submission will be updated soon.
</p>


## **Invited Speakers**

TBA

## **Organisers**
<a id="organizers"></a>

- Wei Emma Zhang, Adelaide University
- Xiang Dai, CSIRO
- Sarvnaz Karimi, CSIRO
- Desmond Elliot, University of Copenhagen
- Byron Fang, Oracle
- Mong Yuan Sim, Adelaide University & CSIRO

## **Previous Edition**
- <a href="/2025/">EvalMG25 @ COLING 2025</a>
