---
layout: default
---


## **The First Workshop of Evaluation of Multi-Modal Generation**

<p style="text-align: justify;">
    Multimodal generation techniques have opened new avenues for creative content generation.  However, evaluating the quality of multimodal generation remains underexplored and some key questions are unanswered, such as the contributions of each modal, the utility of pre-trained large language models for multimodal generation, and measuring faithfulness and fairness in multimodal outputs.  This workshop aims to foster discussions and research efforts by bringing together researchers and practitioners in natural language processing, computer vision, and multimodal AI. Our goal is to establish evaluation methods for multimodal research and advance research efforts in this direction. 
</p>

## **Schedule**

<b>Date: 20 January 2025 (Monday)</b>
<br>
<b>Venue: Abu Dahbi National Exhibition Center , Capital Suite 10</b>
<br>
<i>All times are Abu Dhabi local time, Gulf Standard Time (GST), UTC+4</i>

<table>
  <tr style="background-color: #807e7e; color: white;">
    <th style="width: 180px;">Time</th>
    <th>Presentation Details</th>
  </tr>
  <tr>
    <td>9:00 - 9:10</td>
    <td>Opening</td>
  </tr>
  <tr>
    <td>9:10 - 10:10</td>
    <td><b>Keynote I - A/Prof Qi Wu</b><br>Topic: Reasoning is Measurable: Two new evaluation datasets & metrics on LLMs and MLLMs</td>
  </tr>
  <tr>
    <td>10:10 - 10:30</td>
    <td><b>Paper presentation</b><br>CVT5: Using Compressed Video Encoder and UMT5 for Dense Video Captioning<br><i>Authors: Mohammad Javad Pirhadi, Motahhare Mirzaei and Sauleh Eetemadi</i></td>
  </tr>
  <tr>
    <td>10:30 - 11:00</td>
    <td>Conference tea break</td>
  </tr>
  <tr>
    <td>11:00 - 12:00</td>
    <td><b>Keynote II - Prof Timothy Baldwin</b><br>Topic: Evaluating The "Humanism" of Foundation Models: Culture and Safety</td>
  </tr>
  <tr>
    <td>12:00 - 12:40</td>
    <td><b>Paper presentation</b><br>TaiwanVQA: A Benchmark for Visual Question Answering for Taiwanese Daily Life<br><i>Authors: Hsin-Yi Hsieh, Shang Wei Liu, Chang Chih Meng, Shuo-Yueh Lin, Chen Chien-Hua, Hung-Ju Lin, Hen-Hsen Huang and I-Chen Wu</i><br><br>Persian in a Court: Benchmarking VLMs In Persian Multi-Modal Tasks<br><i>Authors: Farhan Farsi, Shahriar Shariati Motlagh, Shayan Bali, Sadra Sabouri and Saeedeh Momtazi</i><br><br>(Invited) ACE-M^3: Automatic Capability Evaluator for Multimodal Medical Models <br><i>Authors: Xiechi Zhang, Shunfan Zheng, Linlin Wang, Gerard de Melo, Zhu Cao, xiaoling Wang and Liang He</i></td>
  </tr>
  <tr>
    <td>13:00 - 14:00</td>
@@ -64,28 +64,28 @@
  </tr>
  <tr>
    <td>16:00 - 16:50</td>
    <td><b>Papers presentation</b><br>If I feel smart, I will do the right thing: Combining Complementary Multimodal Information in Visual Language Models<br><i>Authors: Yuyu Bai and Sandro Pezzelle</i><br><br>A Dataset for Programming-based Instructional Video Classification and Question Answering<br><i>Authors: Sana Javaid Raja, Adeel Zafar and Aqsa Shoaib</i><br><br>LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model<br><i>Authors: Tao Sun, Oliver Liu, JinJin Li and Lan Ma</i></td>
  </tr>
</table>

## **Venue**
<b>Venue: Abu Dahbi National Exhibition Center , Capital Suite 10</b>
<br>


 <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3632.94219802992!2d54.434064711021115!3d24.418075578129958!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x3e5e42742c04a181%3A0xc2f73ba3f513d4b3!2sADNEC%20Centre%20Abu%20Dhabi!5e0!3m2!1sen!2smy!4v1737189684815!5m2!1sen!2smy" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>


## **Call for Papers**
<a id="call-for-papers"></a>

Both long paper and short papers (up to 8 pages and 4 pages respectively with unlimited references and appendices) are welcomed for submission. 

A list of topics relevant to this workshop (but not limited to):

- Evaluation metrics for multimodal text generation for assessing informativeness, factuality and faithfulness 

- New benchmark datasets, evaluation protocols and annotations

- Challenges in evaluating multimodal coherence, relevance and contribution of modalities and inter- and intra-interactions
